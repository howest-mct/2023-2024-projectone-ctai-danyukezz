{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def main():\n",
    "    model = YOLO(model=\"yolov8s.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/face_detection-2/data.yaml\", epochs=5, imgsz=(640, 640), verbose=True, batch=8)\n",
    "    model.val()\n",
    "    model.export()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"ai-emotion-detection-music-bot\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 üöÄ Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5, epochs=50, time=None, patience=100, batch=8, imgsz=(48, 48), save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train6\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... found 10640 images in 4 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... found 765 images in 4 classes ‚úÖ \n",
      "Overriding model.yaml nc=1000 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    335364  ultralytics.nn.modules.head.Classify         [256, 4]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1443412 parameters, 1443412 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train6', view at http://localhost:6006/\n",
      "WARNING ‚ö†Ô∏è updating to 'imgsz=48'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "WARNING ‚ö†Ô∏è imgsz=[48] must be multiple of max stride 32, updating to [64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... 10640 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10640/10640 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... 765 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 765/765 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 64 train, 64 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train6\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.371          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [02:11<00:00, 10.14it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:09<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.439          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.265          8         64:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 595/1330 [01:08<01:17,  9.49it/s]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "def main():\n",
    "    model = YOLO(model=\"yolov8n-cls.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5\", epochs=50, imgsz=(48,48), verbose=True, batch=8)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to take a screenshot from the camera feed\n",
    "def take_screenshot():\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Save the frame as an image\n",
    "        cv2.imwrite('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg', frame)\n",
    "    else:\n",
    "        print(\"Failed to capture frame from the camera feed\")\n",
    "\n",
    "# Start the camera feed\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to the appropriate camera index if needed\n",
    "\n",
    "# Load your trained model\n",
    "# model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/runs/detect/train5/weights/best.pt\")\n",
    "\n",
    "# Listen for key presses\n",
    "while True:\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(' '):  # Space key press\n",
    "        take_screenshot()\n",
    "        break\n",
    "    elif key == ord('q'):  # Q key press\n",
    "        break\n",
    "\n",
    "# Close the camera feed and all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg: 384x640 2 faces, 119.1ms\n",
      "Speed: 1.6ms preprocess, 119.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAwADABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AJdNtp/Gmo3N9qDOLGM4hh6cVqz6HplohSG2iAA4+UVyWreH7C4Y/ulVvVRiufbwnAx/1hx9KJPCtoU2nOcda1fC2s3HgfVIYJGaTSrlwsgz9w+temeGFtNM8LLe3DqiMveuW1HxtoazOonkPPaNv8KzhqtrqOXt3LD3BB/WopriGGMu5wBWNL4l0yNyhlcH12HFVtUuYdS0lpLZxIAw6dc164Wa28IWMSRBwYt2WGQM15b4hjvpMeRJEJd3JRMDFb3grRdRuJD9sJNuezLyax/EcM0V5LBbcKGPvXPQW2qmRvmjZP4Qyj9a6jRbHzJ7eKSJFd54s4HBw4r126jiTRWtmGFQHZ7D0rynUr2GG8McSB5M9K67RtRjh0hRNMscrDJUqQc+ma4bVrpDfSMG+cse2aZaXUbkCRQGrq/C2m/2rq6jOI4cSOfoRgV0y6k9/CUdFB2/MVrhptAvYtZaW28oNyyNMu5SaZfTSyWvkavEbW5Xq0ceY269MHI7Vy96VhbFuRO5HB2FR+tPs4JmZXmILD+6MCu00C8udNVmtnZZHba34VtaQQVdD0PBqkmsi2upbG9XLxHBB6kdmH4VhazO8wKC5LxE/KTjIHpWCsMSyZ3FvpzV7SG+06hKQB5UCc4/vHp+gNdHb25hsi+PnkIAAAE/QV//2Q==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Roboflow and download the model\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "\n",
    "model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/detect/train7/weights/best.pt\")\n",
    "\n",
    "\n",
    "path = \"screenshot.jpg\"\n",
    "result = model.predict(path)\n",
    "\n",
    "\n",
    "bounding_box = result[0].boxes[0]  \n",
    "\n",
    "x1, y1, x2, y2 = bounding_box.xyxy[0].tolist()\n",
    "\n",
    "original_image = PILImage.open(path)\n",
    "\n",
    "cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "cropped_image = cropped_image.resize((48,48), resample=Image.BILINEAR)\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "cropped_image_np = np.array(cropped_image)\n",
    "\n",
    "# Convert the image from RGB to BGR (if needed)\n",
    "cropped_image_np = cv2.cvtColor(cropped_image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Convert the BGR image to grayscale\n",
    "gray_image = cv2.cvtColor(cropped_image_np, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the grayscale image back to PIL Image\n",
    "gray_image_pil = Image.fromarray(gray_image)\n",
    "\n",
    "# Save the grayscale image\n",
    "gray_image_pil.save(\"cropped_face.jpg\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"cropped_face.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import supervision as sv\n",
    "\n",
    "# print(model.names)\n",
    "# def get_keys_from_value(d, val):\n",
    "#     return [k for k, v in d.items() if v == val]\n",
    "\n",
    "# detections = sv.Detections.from_ultralytics(result)\n",
    "# detections = detections[detections.class_id == get_keys_from_value(model.names, 'face')] # if necessary replace by your own class\n",
    "# print(\"Amount of faces: \",len(detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL.Image as Image\n",
    "# import gradio as gr\n",
    "\n",
    "# from ultralytics import ASSETS\n",
    "\n",
    "# def predict_image(img, conf_threshold, iou_threshold):\n",
    "#     results = model.predict(\n",
    "#         source=img,\n",
    "#         conf=conf_threshold,\n",
    "#         iou=iou_threshold,\n",
    "#         show_labels=True,\n",
    "#         show_conf=True,\n",
    "#         imgsz=640,\n",
    "#         agnostic_nms=True\n",
    "#     )\n",
    "\n",
    "#     for r in results:\n",
    "#         im_array = r.plot()\n",
    "#         im = Image.fromarray(im_array[..., ::-1])\n",
    "\n",
    "#     result = model.predict(img)\n",
    "#     result = result[0] if isinstance(result, list) else result \n",
    "#     detections = sv.Detections.from_ultralytics(result)\n",
    "#     detections = detections[detections.class_id == get_keys_from_value(model.names, 'Drone')]\n",
    "\n",
    "#     print(results)\n",
    "#     return im, {len(detections)}\n",
    "\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=predict_image,\n",
    "#     inputs=[\n",
    "#         gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence threshold\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.45, label=\"IoU threshold\")\n",
    "#     ],\n",
    "#     outputs=[gr.Image(type=\"pil\", label=\"Result\"), gr.Textbox(label=\"Amount of drones\")],\n",
    "#     title=\"Ultralytics Gradio\",\n",
    "#     description=\"Upload images for inference. The Ultralytics YOLOv8n model is used by default.\",\n",
    "#     theme = gr.themes.Monochrome()\n",
    "\n",
    "# )\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "{\n",
      "  \"time\": 0.10144313199998578,\n",
      "  \"image\": {\n",
      "    \"width\": 48,\n",
      "    \"height\": 48\n",
      "  },\n",
      "  \"predictions\": {\n",
      "    \"angry\": {\n",
      "      \"confidence\": 0.019471323117613792\n",
      "    },\n",
      "    \"happy\": {\n",
      "      \"confidence\": 0.9804863929748535\n",
      "    },\n",
      "    \"neutral\": {\n",
      "      \"confidence\": 0.022298969328403473\n",
      "    },\n",
      "    \"sad\": {\n",
      "      \"confidence\": 0.018164386972784996\n",
      "    }\n",
      "  },\n",
      "  \"predicted_classes\": [\n",
      "    \"happy\"\n",
      "  ],\n",
      "  \"image_path\": \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\",\n",
      "  \"prediction_type\": \"ClassificationModel\"\n",
      "}\n",
      "\n",
      "\n",
      "Predicted Emotion with Highest Confidence: happy, Confidence Score: 0.9804863929748535\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "model = project.version(1).model\n",
    "\n",
    "# infer on a local image\n",
    "print(model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\"))\n",
    "\n",
    "# visualize your prediction\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "prediction = predictions[0]\n",
    "\n",
    "# Extract predicted emotions and their confidence scores\n",
    "predicted_emotions = prediction['predictions']\n",
    "\n",
    "# Find the emotion with the highest confidence score\n",
    "max_confidence = 0\n",
    "predicted_emotion = None\n",
    "\n",
    "for emotion, info in predicted_emotions.items():\n",
    "    confidence_score = info['confidence']\n",
    "    if confidence_score > max_confidence:\n",
    "        max_confidence = confidence_score\n",
    "        predicted_emotion = emotion\n",
    "\n",
    "# Print the predicted emotion with the highest confidence score\n",
    "print(f\"Predicted Emotion with Highest Confidence: {predicted_emotion}, Confidence Score: {max_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg: 64x64 happy 0.99, neutral 0.01, sad 0.00, angry 0.00, 2.6ms\n",
      "Speed: 1.1ms preprocess, 2.6ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: None\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'angry', 1: 'happy', 2: 'neutral', 3: 'sad'}\n",
      "obb: None\n",
      "orig_img: array([[[199, 199, 199],\n",
      "        [209, 209, 209],\n",
      "        [199, 199, 199],\n",
      "        ...,\n",
      "        [223, 223, 223],\n",
      "        [211, 211, 211],\n",
      "        [212, 212, 212]],\n",
      "\n",
      "       [[207, 207, 207],\n",
      "        [188, 188, 188],\n",
      "        [152, 152, 152],\n",
      "        ...,\n",
      "        [201, 201, 201],\n",
      "        [208, 208, 208],\n",
      "        [211, 211, 211]],\n",
      "\n",
      "       [[198, 198, 198],\n",
      "        [148, 148, 148],\n",
      "        [ 90,  90,  90],\n",
      "        ...,\n",
      "        [165, 165, 165],\n",
      "        [203, 203, 203],\n",
      "        [221, 221, 221]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[134, 134, 134],\n",
      "        [132, 132, 132],\n",
      "        [137, 137, 137],\n",
      "        ...,\n",
      "        [143, 143, 143],\n",
      "        [110, 110, 110],\n",
      "        [ 99,  99,  99]],\n",
      "\n",
      "       [[136, 136, 136],\n",
      "        [131, 131, 131],\n",
      "        [134, 134, 134],\n",
      "        ...,\n",
      "        [147, 147, 147],\n",
      "        [123, 123, 123],\n",
      "        [121, 121, 121]],\n",
      "\n",
      "       [[136, 136, 136],\n",
      "        [130, 130, 130],\n",
      "        [132, 132, 132],\n",
      "        ...,\n",
      "        [122, 122, 122],\n",
      "        [139, 139, 139],\n",
      "        [169, 169, 169]]], dtype=uint8)\n",
      "orig_shape: (48, 48)\n",
      "path: '/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg'\n",
      "probs: ultralytics.engine.results.Probs object\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 1.1398792266845703, 'inference': 2.5789737701416016, 'postprocess': 0.03528594970703125}]\n",
      "Predicted Emotion: happy, Confidence Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow  # Assuming the YOLO class is in the yolov5 module\n",
    "import torch\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "\n",
    "# Get project from Roboflow workspace\n",
    "\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "# model = project.version(1).model\n",
    "\n",
    "# Define the path to the weights file\n",
    "weights_path = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train6/weights/best.pt\"\n",
    "\n",
    "# Initialize YOLO model with the weights file\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Perform inference on the image\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "print(predictions)\n",
    "\n",
    "class_labels = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "# Extract the class probabilities from the predictions\n",
    "prediction = predictions[0]  # Assuming a single prediction\n",
    "class_probs = prediction.probs\n",
    "\n",
    "# Find the index of the highest confidence score\n",
    "top1_index = class_probs.top1\n",
    "top1_confidence = class_probs.top1conf\n",
    "\n",
    "# Map the index to the corresponding class label\n",
    "predicted_emotion = class_labels[top1_index]\n",
    "\n",
    "# Print the predicted emotion and its confidence score\n",
    "print(f\"Predicted Emotion: {predicted_emotion}, Confidence Score: {top1_confidence.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Justin Timberlake\n",
      "Song:  Can't Stop the Feeling!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Load the CSV file and store the songs in a dictionary\n",
    "songs_by_emotion = {1: [], 2: [], 3: [], 4: []}\n",
    "\n",
    "with open('music.csv', mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        emotion = int(row['Emotion'])\n",
    "        author = row['Author Name']\n",
    "        song = row['Song Name']\n",
    "        songs_by_emotion[emotion].append((author, song))\n",
    "\n",
    "# Map the predicted emotion to the corresponding emotion code\n",
    "if predicted_emotion == 'neutral':\n",
    "    emotion_code = 1\n",
    "elif predicted_emotion == 'happy':\n",
    "    emotion_code = 2\n",
    "elif predicted_emotion == 'sad':\n",
    "    emotion_code = 3\n",
    "elif predicted_emotion == 'angry':\n",
    "    emotion_code = 4\n",
    "\n",
    "# Get a random song from the list of songs corresponding to the predicted emotion\n",
    "random_author, random_song = random.choice(songs_by_emotion[emotion_code])\n",
    "print(\"Author:\", random_author)\n",
    "print(\"Song:\", random_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lcd import LCD\n",
    "\n",
    "# def get_spaces(spaces):\n",
    "#       string = ''\n",
    "#       for i in range(16 - spaces):\n",
    "#          string += ' '\n",
    "#       return string\n",
    "\n",
    "# # Initialize LCD and display the author and the song\n",
    "# lcd = LCD()\n",
    "# lcd.lcd_init()\n",
    "# lcd.send_string(f\"{random_author+get_spaces(len(random_author))}\", lcd.LCD_LINE_1)\n",
    "# lcd.send_string(f\"{random_song+get_spaces(len(random_song))}\", lcd.LCD_LINE_2)\n",
    "# time.sleep(5)\n",
    "# lcd.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Can't Stop the Feeling!\n",
      "Loading file: /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs/Justin Timberlake Can't Stop The Feeling! (From Dreamworks Animation's _Trolls_).mp3\n",
      "Press Enter to stop playback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 7.0.1 Copyright (c) 2003-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[wav @ 0x130108290] Ignoring maximum wav data size, file may be invalid\n",
      "Input #0, wav, from 'fd:':\n",
      "  Metadata:\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: N/A, bitrate: 1411 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, 2 channels, s16, 1411 kb/s\n",
      "   6.37 M-A: -0.000 fd=   0 aq=  416KB vq=    0KB sq=    0B \r"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Directory containing the MP3 files\n",
    "directory = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs\"  # Change this to your directory\n",
    "\n",
    "# Change this to your actual random song variable\n",
    "print(random_song)\n",
    "\n",
    "# List all files in the directory\n",
    "files_in_directory = os.listdir(directory)\n",
    "\n",
    "# Filter files that contain the song name as a substring\n",
    "matching_files = [file for file in files_in_directory if random_song.lower() in file.lower()]\n",
    "\n",
    "# Print the matching files\n",
    "if matching_files:\n",
    "    file_path = os.path.join(directory, matching_files[0])\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    mp3_file = file_path\n",
    "    \n",
    "    # Command for ffmpeg to convert MP3 to WAV and output to stdout\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \n",
    "        \"-i\", mp3_file, \n",
    "        \"-f\", \"wav\", \n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    # Command for ffplay to read WAV audio from stdin\n",
    "    ffplay_command = [\n",
    "        \"ffplay\", \n",
    "        \"-nodisp\",  # Suppress video display\n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Start ffmpeg subprocess to convert MP3 to WAV and output to stdout\n",
    "        ffmpeg_process = subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "        # Start ffplay subprocess to read WAV audio from stdin\n",
    "        ffplay_process = subprocess.Popen(ffplay_command, stdin=ffmpeg_process.stdout)\n",
    "        \n",
    "        # Wait for the user to exit by pressing Enter\n",
    "        print(\"Press Enter to stop playback...\")\n",
    "        input() # Wait for a single character input (Enter key)\n",
    "        \n",
    "        # Terminate ffplay process\n",
    "        ffplay_process.terminate()\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"No matching files found for the song: {random_song}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
