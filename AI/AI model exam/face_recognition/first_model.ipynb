{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def main():\n",
    "    model = YOLO(model=\"yolov8s.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/face_detection-2/data.yaml\", epochs=5, imgsz=(640, 640), verbose=True, batch=8)\n",
    "    model.val()\n",
    "    model.export()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"ai-emotion-detection-music-bot\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 üöÄ Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5, epochs=50, time=None, patience=100, batch=8, imgsz=(48, 48), save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train7\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... found 10640 images in 4 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... found 765 images in 4 classes ‚úÖ \n",
      "Overriding model.yaml nc=1000 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    335364  ultralytics.nn.modules.head.Classify         [256, 4]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1443412 parameters, 1443412 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train7', view at http://localhost:6006/\n",
      "WARNING ‚ö†Ô∏è updating to 'imgsz=48'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "WARNING ‚ö†Ô∏è imgsz=[48] must be multiple of max stride 32, updating to [64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... 10640 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10640/10640 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... 765 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 765/765 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 64 train, 64 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train7\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.371          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:16<00:00, 17.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.439          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.236          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 20.01it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:04<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.489          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G      1.174          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.89it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.531          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.134          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.548          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.092          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.578          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.044          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.92it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.588          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      1.031          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:03<00:00, 20.84it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.601          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G     0.9976          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 20.09it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:07<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.629          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G     0.9802          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.83it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.624          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G     0.9653          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.98it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.644          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G     0.9387          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:17<00:00, 17.05it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.635          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G     0.9218          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.73it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G     0.9138          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.40it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G     0.8989          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.634          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G     0.8881          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:05<00:00, 20.28it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.639          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G     0.8706          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.647          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G     0.8649          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.68it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G     0.8494          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.39it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.642          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G     0.8355          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.09it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.642          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G     0.8347          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.80it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.655          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G     0.8263          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:05<00:00, 20.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.643          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G     0.8114          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.97it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.634          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G     0.8059          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.62it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.638          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G     0.7848          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:04<00:00, 20.69it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.659          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G     0.7967          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:56<00:00, 11.39it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.641          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G     0.7878          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:26<00:00, 15.34it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.658          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G     0.7688          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:13<00:00, 18.20it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.642          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G     0.7728          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:15<00:00, 17.68it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      0.751          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:15<00:00, 17.68it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G      0.749          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:14<00:00, 17.94it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G     0.7538          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.60it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.656          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G     0.7321          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.73it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.647          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G     0.7178          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.66it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.646          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G       0.73          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.74it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G     0.7131          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.19it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G     0.7042          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.35it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G     0.7035          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.72it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.66          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G     0.6993          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.29it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.652          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G     0.6925          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.15it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.661          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G     0.6856          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.16it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50         0G     0.6953          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.18it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.654          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G     0.6778          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.00it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.661          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G     0.6704          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:12<00:00, 18.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.665          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G      0.665          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:16<00:00, 17.45it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.671          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G     0.6762          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.661          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G     0.6592          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.79it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.664          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G     0.6664          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.61it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.667          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G     0.6416          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.652          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G     0.6508          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G     0.6485          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.65it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 1.064 hours.\n",
      "Optimizer stripped from runs/classify/train7/weights/last.pt, 3.0MB\n",
      "Optimizer stripped from runs/classify/train7/weights/best.pt, 3.0MB\n",
      "\n",
      "Validating runs/classify/train7/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.34 üöÄ Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1440004 parameters, 0 gradients, 3.3 GFLOPs\n",
      "WARNING ‚ö†Ô∏è Dataset 'split=val' not found, using 'split=test' instead.\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... found 10640 images in 4 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... found 765 images in 4 classes ‚úÖ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.669          1\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n",
      "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "def main():\n",
    "    model = YOLO(model=\"yolov8n-cls.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5\", epochs=50, imgsz=(48,48), verbose=True, batch=8)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to take a screenshot from the camera feed\n",
    "def take_screenshot():\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Save the frame as an image\n",
    "        cv2.imwrite('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg', frame)\n",
    "    else:\n",
    "        print(\"Failed to capture frame from the camera feed\")\n",
    "\n",
    "# Start the camera feed\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to the appropriate camera index if needed\n",
    "\n",
    "# Load your trained model\n",
    "# model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/runs/detect/train5/weights/best.pt\")\n",
    "\n",
    "# Listen for key presses\n",
    "while True:\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(' '):  # Space key press\n",
    "        take_screenshot()\n",
    "        break\n",
    "    elif key == ord('q'):  # Q key press\n",
    "        break\n",
    "\n",
    "# Close the camera feed and all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg: 384x640 1 face, 114.2ms\n",
      "Speed: 1.5ms preprocess, 114.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Roboflow and download the model\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "\n",
    "model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/detect/train7/weights/best.pt\")\n",
    "\n",
    "\n",
    "path = \"screenshot.jpg\"\n",
    "result = model.predict(path)\n",
    "\n",
    "\n",
    "bounding_box = result[0].boxes[0]  \n",
    "\n",
    "x1, y1, x2, y2 = bounding_box.xyxy[0].tolist()\n",
    "\n",
    "original_image = PILImage.open(path)\n",
    "\n",
    "cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "# cropped_image = cropped_image.resize((48,48), resample=Image.BILINEAR)\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "cropped_image_np = np.array(cropped_image)\n",
    "\n",
    "# Convert the image from RGB to BGR (if needed)\n",
    "cropped_image_np = cv2.cvtColor(cropped_image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Convert the BGR image to grayscale\n",
    "gray_image = cv2.cvtColor(cropped_image_np, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the grayscale image back to PIL Image\n",
    "gray_image_pil = Image.fromarray(gray_image)\n",
    "\n",
    "# Save the grayscale image\n",
    "gray_image_pil.save(\"cropped_face.jpg\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"cropped_face.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import supervision as sv\n",
    "\n",
    "# print(model.names)\n",
    "# def get_keys_from_value(d, val):\n",
    "#     return [k for k, v in d.items() if v == val]\n",
    "\n",
    "# detections = sv.Detections.from_ultralytics(result)\n",
    "# detections = detections[detections.class_id == get_keys_from_value(model.names, 'face')] # if necessary replace by your own class\n",
    "# print(\"Amount of faces: \",len(detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL.Image as Image\n",
    "# import gradio as gr\n",
    "\n",
    "# from ultralytics import ASSETS\n",
    "\n",
    "# def predict_image(img, conf_threshold, iou_threshold):\n",
    "#     results = model.predict(\n",
    "#         source=img,\n",
    "#         conf=conf_threshold,\n",
    "#         iou=iou_threshold,\n",
    "#         show_labels=True,\n",
    "#         show_conf=True,\n",
    "#         imgsz=640,\n",
    "#         agnostic_nms=True\n",
    "#     )\n",
    "\n",
    "#     for r in results:\n",
    "#         im_array = r.plot()\n",
    "#         im = Image.fromarray(im_array[..., ::-1])\n",
    "\n",
    "#     result = model.predict(img)\n",
    "#     result = result[0] if isinstance(result, list) else result \n",
    "#     detections = sv.Detections.from_ultralytics(result)\n",
    "#     detections = detections[detections.class_id == get_keys_from_value(model.names, 'Drone')]\n",
    "\n",
    "#     print(results)\n",
    "#     return im, {len(detections)}\n",
    "\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=predict_image,\n",
    "#     inputs=[\n",
    "#         gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence threshold\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.45, label=\"IoU threshold\")\n",
    "#     ],\n",
    "#     outputs=[gr.Image(type=\"pil\", label=\"Result\"), gr.Textbox(label=\"Amount of drones\")],\n",
    "#     title=\"Ultralytics Gradio\",\n",
    "#     description=\"Upload images for inference. The Ultralytics YOLOv8n model is used by default.\",\n",
    "#     theme = gr.themes.Monochrome()\n",
    "\n",
    "# )\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "{\n",
      "  \"time\": 0.12522110300005806,\n",
      "  \"image\": {\n",
      "    \"width\": 398,\n",
      "    \"height\": 272\n",
      "  },\n",
      "  \"predictions\": {\n",
      "    \"angry\": {\n",
      "      \"confidence\": 0.01512478943914175\n",
      "    },\n",
      "    \"happy\": {\n",
      "      \"confidence\": 0.9799479246139526\n",
      "    },\n",
      "    \"neutral\": {\n",
      "      \"confidence\": 0.024581139907240868\n",
      "    },\n",
      "    \"sad\": {\n",
      "      \"confidence\": 0.018005594611167908\n",
      "    }\n",
      "  },\n",
      "  \"predicted_classes\": [\n",
      "    \"happy\"\n",
      "  ],\n",
      "  \"image_path\": \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\",\n",
      "  \"prediction_type\": \"ClassificationModel\"\n",
      "}\n",
      "\n",
      "\n",
      "Predicted Emotion with Highest Confidence: happy, Confidence Score: 0.9799362421035767\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "model = project.version(1).model\n",
    "\n",
    "# infer on a local image\n",
    "print(model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\"))\n",
    "\n",
    "# visualize your prediction\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "prediction = predictions[0]\n",
    "\n",
    "# Extract predicted emotions and their confidence scores\n",
    "predicted_emotions = prediction['predictions']\n",
    "\n",
    "# Find the emotion with the highest confidence score\n",
    "max_confidence = 0\n",
    "predicted_emotion = None\n",
    "\n",
    "for emotion, info in predicted_emotions.items():\n",
    "    confidence_score = info['confidence']\n",
    "    if confidence_score > max_confidence:\n",
    "        max_confidence = confidence_score\n",
    "        predicted_emotion = emotion\n",
    "\n",
    "# Print the predicted emotion with the highest confidence score\n",
    "print(f\"Predicted Emotion with Highest Confidence: {predicted_emotion}, Confidence Score: {max_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg: 64x64 happy 0.97, neutral 0.03, sad 0.00, angry 0.00, 3.0ms\n",
      "Speed: 1.8ms preprocess, 3.0ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: None\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'angry', 1: 'happy', 2: 'neutral', 3: 'sad'}\n",
      "obb: None\n",
      "orig_img: array([[[196, 196, 196],\n",
      "        [196, 196, 196],\n",
      "        [196, 196, 196],\n",
      "        ...,\n",
      "        [195, 195, 195],\n",
      "        [194, 194, 194],\n",
      "        [194, 194, 194]],\n",
      "\n",
      "       [[196, 196, 196],\n",
      "        [196, 196, 196],\n",
      "        [196, 196, 196],\n",
      "        ...,\n",
      "        [195, 195, 195],\n",
      "        [194, 194, 194],\n",
      "        [194, 194, 194]],\n",
      "\n",
      "       [[196, 196, 196],\n",
      "        [196, 196, 196],\n",
      "        [196, 196, 196],\n",
      "        ...,\n",
      "        [195, 195, 195],\n",
      "        [194, 194, 194],\n",
      "        [194, 194, 194]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[181, 181, 181],\n",
      "        [195, 195, 195],\n",
      "        [199, 199, 199],\n",
      "        ...,\n",
      "        [132, 132, 132],\n",
      "        [132, 132, 132],\n",
      "        [132, 132, 132]],\n",
      "\n",
      "       [[180, 180, 180],\n",
      "        [195, 195, 195],\n",
      "        [199, 199, 199],\n",
      "        ...,\n",
      "        [133, 133, 133],\n",
      "        [134, 134, 134],\n",
      "        [133, 133, 133]],\n",
      "\n",
      "       [[180, 180, 180],\n",
      "        [194, 194, 194],\n",
      "        [199, 199, 199],\n",
      "        ...,\n",
      "        [134, 134, 134],\n",
      "        [133, 133, 133],\n",
      "        [132, 132, 132]]], dtype=uint8)\n",
      "orig_shape: (398, 272)\n",
      "path: '/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg'\n",
      "probs: ultralytics.engine.results.Probs object\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 1.8498897552490234, 'inference': 3.0090808868408203, 'postprocess': 0.03719329833984375}]\n",
      "Predicted Emotion: happy, Confidence Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow  # Assuming the YOLO class is in the yolov5 module\n",
    "import torch\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "\n",
    "# Get project from Roboflow workspace\n",
    "\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "# model = project.version(1).model\n",
    "\n",
    "# Define the path to the weights file\n",
    "weights_path = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train7/weights/best.pt\"\n",
    "\n",
    "# Initialize YOLO model with the weights file\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Perform inference on the image\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "print(predictions)\n",
    "\n",
    "class_labels = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "# Extract the class probabilities from the predictions\n",
    "prediction = predictions[0]  # Assuming a single prediction\n",
    "class_probs = prediction.probs\n",
    "\n",
    "# Find the index of the highest confidence score\n",
    "top1_index = class_probs.top1\n",
    "top1_confidence = class_probs.top1conf\n",
    "\n",
    "# Map the index to the corresponding class label\n",
    "predicted_emotion = class_labels[top1_index]\n",
    "\n",
    "# Print the predicted emotion and its confidence score\n",
    "print(f\"Predicted Emotion: {predicted_emotion}, Confidence Score: {top1_confidence.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Justin Timberlake\n",
      "Song:  Can't Stop the Feeling!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Load the CSV file and store the songs in a dictionary\n",
    "songs_by_emotion = {1: [], 2: [], 3: [], 4: []}\n",
    "\n",
    "with open('music.csv', mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        emotion = int(row['Emotion'])\n",
    "        author = row['Author Name']\n",
    "        song = row['Song Name']\n",
    "        songs_by_emotion[emotion].append((author, song))\n",
    "\n",
    "# Map the predicted emotion to the corresponding emotion code\n",
    "if predicted_emotion == 'neutral':\n",
    "    emotion_code = 1\n",
    "elif predicted_emotion == 'happy':\n",
    "    emotion_code = 2\n",
    "elif predicted_emotion == 'sad':\n",
    "    emotion_code = 3\n",
    "elif predicted_emotion == 'angry':\n",
    "    emotion_code = 4\n",
    "\n",
    "# Get a random song from the list of songs corresponding to the predicted emotion\n",
    "random_author, random_song = random.choice(songs_by_emotion[emotion_code])\n",
    "print(\"Author:\", random_author)\n",
    "print(\"Song:\", random_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lcd import LCD\n",
    "\n",
    "# def get_spaces(spaces):\n",
    "#       string = ''\n",
    "#       for i in range(16 - spaces):\n",
    "#          string += ' '\n",
    "#       return string\n",
    "\n",
    "# # Initialize LCD and display the author and the song\n",
    "# lcd = LCD()\n",
    "# lcd.lcd_init()\n",
    "# lcd.send_string(f\"{random_author+get_spaces(len(random_author))}\", lcd.LCD_LINE_1)\n",
    "# lcd.send_string(f\"{random_song+get_spaces(len(random_song))}\", lcd.LCD_LINE_2)\n",
    "# time.sleep(5)\n",
    "# lcd.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Can't Stop the Feeling!\n",
      "Loading file: /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs/Justin Timberlake Can't Stop The Feeling! (From Dreamworks Animation's _Trolls_).mp3\n",
      "Press Enter to stop playback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 7.0.1 Copyright (c) 2003-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[wav @ 0x1599190e0] Ignoring maximum wav data size, file may be invalid\n",
      "Input #0, wav, from 'fd:':0 aq=    0KB vq=    0KB sq=    0B \n",
      "  Metadata:\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: N/A, bitrate: 1411 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, 2 channels, s16, 1411 kb/s\n",
      "  16.53 M-A:  0.000 fd=   0 aq=  416KB vq=    0KB sq=    0B \r"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Directory containing the MP3 files\n",
    "directory = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs\"  # Change this to your directory\n",
    "\n",
    "# Change this to your actual random song variable\n",
    "print(random_song)\n",
    "\n",
    "# List all files in the directory\n",
    "files_in_directory = os.listdir(directory)\n",
    "\n",
    "# Filter files that contain the song name as a substring\n",
    "matching_files = [file for file in files_in_directory if random_song.lower() in file.lower()]\n",
    "\n",
    "# Print the matching files\n",
    "if matching_files:\n",
    "    file_path = os.path.join(directory, matching_files[0])\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    mp3_file = file_path\n",
    "    \n",
    "    # Command for ffmpeg to convert MP3 to WAV and output to stdout\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \n",
    "        \"-i\", mp3_file, \n",
    "        \"-f\", \"wav\", \n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    # Command for ffplay to read WAV audio from stdin\n",
    "    ffplay_command = [\n",
    "        \"ffplay\", \n",
    "        \"-nodisp\",  # Suppress video display\n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Start ffmpeg subprocess to convert MP3 to WAV and output to stdout\n",
    "        ffmpeg_process = subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "        # Start ffplay subprocess to read WAV audio from stdin\n",
    "        ffplay_process = subprocess.Popen(ffplay_command, stdin=ffmpeg_process.stdout)\n",
    "        \n",
    "        # Wait for the user to exit by pressing Enter\n",
    "        print(\"Press Enter to stop playback...\")\n",
    "        input() # Wait for a single character input (Enter key)\n",
    "        \n",
    "        # Terminate ffplay process\n",
    "        ffplay_process.terminate()\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"No matching files found for the song: {random_song}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
