{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def main():\n",
    "    model = YOLO(model=\"yolov8s.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/face_detection-2/data.yaml\", epochs=5, imgsz=(640, 640), verbose=True, batch=8)\n",
    "    model.val()\n",
    "    model.export()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"ai-emotion-detection-music-bot\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 üöÄ Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5, epochs=50, time=None, patience=100, batch=8, imgsz=(48, 48), save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train7\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... found 10640 images in 4 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... found 765 images in 4 classes ‚úÖ \n",
      "Overriding model.yaml nc=1000 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    335364  ultralytics.nn.modules.head.Classify         [256, 4]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1443412 parameters, 1443412 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train7', view at http://localhost:6006/\n",
      "WARNING ‚ö†Ô∏è updating to 'imgsz=48'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "WARNING ‚ö†Ô∏è imgsz=[48] must be multiple of max stride 32, updating to [64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... 10640 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10640/10640 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... 765 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 765/765 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 64 train, 64 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train7\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.371          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:16<00:00, 17.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.439          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.236          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 20.01it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:04<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.489          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G      1.174          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.89it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.531          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.134          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.548          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.092          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.578          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.044          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.92it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.588          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      1.031          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:03<00:00, 20.84it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.601          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G     0.9976          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 20.09it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:07<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.629          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G     0.9802          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.83it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.624          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G     0.9653          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.98it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.644          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G     0.9387          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:17<00:00, 17.05it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.635          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G     0.9218          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.73it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G     0.9138          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.40it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G     0.8989          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:06<00:00, 19.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.634          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G     0.8881          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:05<00:00, 20.28it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.639          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G     0.8706          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.647          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G     0.8649          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.68it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G     0.8494          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.39it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.642          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G     0.8355          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.09it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.642          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G     0.8347          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.80it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.655          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G     0.8263          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:05<00:00, 20.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.643          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G     0.8114          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.97it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.634          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G     0.8059          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.62it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.638          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G     0.7848          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:04<00:00, 20.69it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.659          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G     0.7967          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:56<00:00, 11.39it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.641          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G     0.7878          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:26<00:00, 15.34it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.658          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G     0.7688          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:13<00:00, 18.20it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.642          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G     0.7728          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:15<00:00, 17.68it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      0.751          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:15<00:00, 17.68it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G      0.749          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:14<00:00, 17.94it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G     0.7538          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.60it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.656          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G     0.7321          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.73it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.647          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G     0.7178          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.66it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.646          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G       0.73          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.74it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G     0.7131          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.19it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G     0.7042          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.35it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G     0.7035          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.72it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.66          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G     0.6993          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:08<00:00, 19.29it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.652          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G     0.6925          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.15it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.661          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G     0.6856          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.16it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50         0G     0.6953          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.18it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.654          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G     0.6778          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:09<00:00, 19.00it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.661          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G     0.6704          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:12<00:00, 18.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.665          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G      0.665          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:16<00:00, 17.45it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.671          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G     0.6762          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.661          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G     0.6592          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.79it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.664          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G     0.6664          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.61it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.667          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G     0.6416          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:10<00:00, 18.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.652          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G     0.6508          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:11<00:00, 18.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:06<00:00,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G     0.6485          8         64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1330/1330 [01:07<00:00, 19.65it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.651          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 1.064 hours.\n",
      "Optimizer stripped from runs/classify/train7/weights/last.pt, 3.0MB\n",
      "Optimizer stripped from runs/classify/train7/weights/best.pt, 3.0MB\n",
      "\n",
      "Validating runs/classify/train7/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.34 üöÄ Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1440004 parameters, 0 gradients, 3.3 GFLOPs\n",
      "WARNING ‚ö†Ô∏è Dataset 'split=val' not found, using 'split=test' instead.\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/train... found 10640 images in 4 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5/test... found 765 images in 4 classes ‚úÖ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:05<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.669          1\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n",
      "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "def main():\n",
    "    model = YOLO(model=\"yolov8n-cls.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-5\", epochs=50, imgsz=(48,48), verbose=True, batch=8)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to take a screenshot from the camera feed\n",
    "def take_screenshot():\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Save the frame as an image\n",
    "        cv2.imwrite('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg', frame)\n",
    "    else:\n",
    "        print(\"Failed to capture frame from the camera feed\")\n",
    "\n",
    "# Start the camera feed\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to the appropriate camera index if needed\n",
    "\n",
    "# Load your trained model\n",
    "# model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/runs/detect/train5/weights/best.pt\")\n",
    "\n",
    "# Listen for key presses\n",
    "while True:\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(' '):  # Space key press\n",
    "        take_screenshot()\n",
    "        break\n",
    "    elif key == ord('q'):  # Q key press\n",
    "        break\n",
    "\n",
    "# Close the camera feed and all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg: 384x640 3 faces, 97.8ms\n",
      "Speed: 1.3ms preprocess, 97.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAwADABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/ALms2snxD1PCS+Vo1o21XA5kbuRUy+ENGs4xFHao2OMkZNQSeHNNIJS3VT7VnS+D7IyiQ546ioZvCWlyrtMRB7MDyKueHtSl8E3cdhKTLpl0+7zDyUbHT+Vaum3lp4f8PWxu5RGjDcABkt9AKoS+NrV32w6deuufvFMfzq9b6tFcQiQIVz2bqKo6lrwtk/d2/mk9gcVgjxbdb/3mjTBfVHBput3n9p6G88SunlnLI4wy12epadLJptusHlh0hVdzrntXD3+ha9cSqEugBn5gi4zXTeH9DubCDfdS7ztOc1ia5odze3DSQTlRnhc4rLtPC+qxk7tQcDPcA1vPp7LpNxFI29njKk4x2rrp7owWiLIPnVQDx3Fc1NqstxdNFAdu3qaSXX7u3/dvCghxjIYlv5Vky6xdXE7KIl8s/dbcQ31qzDqrowjkbJ9TV0SyXCtEgLM4IVQM5Ndv4o01niN1GpK4xIB2968yvtNuZWk+y3cls55DL0P1qLz5Y4wl7bs8ijBZJsBuOuCKqXEs0mVtIBHkcM8u4j3wB/WmWOnSxspmuJZmzks54/Cu68LaZPdX4niA2wYYlhxn0r0+UJHbSvMQsaoSxbsK8ovYRDKzKpZD1UdR9KwLgiRiiSnr0bjFVsRwEedONvoDVy3kEzAqhWIdM8E16N4T1fTorK305mKXMrtj5flJ5OMAAM+uBX//2Q==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Roboflow and download the model\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "\n",
    "model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/detect/train7/weights/best.pt\")\n",
    "\n",
    "\n",
    "path = \"screenshot.jpg\"\n",
    "result = model.predict(path)\n",
    "\n",
    "\n",
    "bounding_box = result[0].boxes[0]  \n",
    "\n",
    "x1, y1, x2, y2 = bounding_box.xyxy[0].tolist()\n",
    "\n",
    "original_image = PILImage.open(path)\n",
    "\n",
    "cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "cropped_image = cropped_image.resize((48,48), resample=Image.BILINEAR)\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "cropped_image_np = np.array(cropped_image)\n",
    "\n",
    "# Convert the image from RGB to BGR (if needed)\n",
    "cropped_image_np = cv2.cvtColor(cropped_image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Convert the BGR image to grayscale\n",
    "gray_image = cv2.cvtColor(cropped_image_np, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the grayscale image back to PIL Image\n",
    "gray_image_pil = Image.fromarray(gray_image)\n",
    "\n",
    "# Save the grayscale image\n",
    "gray_image_pil.save(\"cropped_face.jpg\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"cropped_face.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import supervision as sv\n",
    "\n",
    "# print(model.names)\n",
    "# def get_keys_from_value(d, val):\n",
    "#     return [k for k, v in d.items() if v == val]\n",
    "\n",
    "# detections = sv.Detections.from_ultralytics(result)\n",
    "# detections = detections[detections.class_id == get_keys_from_value(model.names, 'face')] # if necessary replace by your own class\n",
    "# print(\"Amount of faces: \",len(detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL.Image as Image\n",
    "# import gradio as gr\n",
    "\n",
    "# from ultralytics import ASSETS\n",
    "\n",
    "# def predict_image(img, conf_threshold, iou_threshold):\n",
    "#     results = model.predict(\n",
    "#         source=img,\n",
    "#         conf=conf_threshold,\n",
    "#         iou=iou_threshold,\n",
    "#         show_labels=True,\n",
    "#         show_conf=True,\n",
    "#         imgsz=640,\n",
    "#         agnostic_nms=True\n",
    "#     )\n",
    "\n",
    "#     for r in results:\n",
    "#         im_array = r.plot()\n",
    "#         im = Image.fromarray(im_array[..., ::-1])\n",
    "\n",
    "#     result = model.predict(img)\n",
    "#     result = result[0] if isinstance(result, list) else result \n",
    "#     detections = sv.Detections.from_ultralytics(result)\n",
    "#     detections = detections[detections.class_id == get_keys_from_value(model.names, 'Drone')]\n",
    "\n",
    "#     print(results)\n",
    "#     return im, {len(detections)}\n",
    "\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=predict_image,\n",
    "#     inputs=[\n",
    "#         gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence threshold\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.45, label=\"IoU threshold\")\n",
    "#     ],\n",
    "#     outputs=[gr.Image(type=\"pil\", label=\"Result\"), gr.Textbox(label=\"Amount of drones\")],\n",
    "#     title=\"Ultralytics Gradio\",\n",
    "#     description=\"Upload images for inference. The Ultralytics YOLOv8n model is used by default.\",\n",
    "#     theme = gr.themes.Monochrome()\n",
    "\n",
    "# )\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "{\n",
      "  \"time\": 0.12543489000000818,\n",
      "  \"image\": {\n",
      "    \"width\": 48,\n",
      "    \"height\": 48\n",
      "  },\n",
      "  \"predictions\": {\n",
      "    \"angry\": {\n",
      "      \"confidence\": 0.01587134785950184\n",
      "    },\n",
      "    \"happy\": {\n",
      "      \"confidence\": 0.9785462021827698\n",
      "    },\n",
      "    \"neutral\": {\n",
      "      \"confidence\": 0.021770678460597992\n",
      "    },\n",
      "    \"sad\": {\n",
      "      \"confidence\": 0.01870732754468918\n",
      "    }\n",
      "  },\n",
      "  \"predicted_classes\": [\n",
      "    \"happy\"\n",
      "  ],\n",
      "  \"image_path\": \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\",\n",
      "  \"prediction_type\": \"ClassificationModel\"\n",
      "}\n",
      "\n",
      "\n",
      "Predicted Emotion with Highest Confidence: happy, Confidence Score: 0.9784168601036072\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "model = project.version(1).model\n",
    "\n",
    "# infer on a local image\n",
    "print(model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\"))\n",
    "\n",
    "# visualize your prediction\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "prediction = predictions[0]\n",
    "\n",
    "# Extract predicted emotions and their confidence scores\n",
    "predicted_emotions = prediction['predictions']\n",
    "\n",
    "# Find the emotion with the highest confidence score\n",
    "max_confidence = 0\n",
    "predicted_emotion = None\n",
    "\n",
    "for emotion, info in predicted_emotions.items():\n",
    "    confidence_score = info['confidence']\n",
    "    if confidence_score > max_confidence:\n",
    "        max_confidence = confidence_score\n",
    "        predicted_emotion = emotion\n",
    "\n",
    "# Print the predicted emotion with the highest confidence score\n",
    "print(f\"Predicted Emotion with Highest Confidence: {predicted_emotion}, Confidence Score: {max_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg: 64x64 happy 1.00, neutral 0.00, angry 0.00, sad 0.00, 5.1ms\n",
      "Speed: 8.2ms preprocess, 5.1ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: None\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'angry', 1: 'happy', 2: 'neutral', 3: 'sad'}\n",
      "obb: None\n",
      "orig_img: array([[[217, 217, 217],\n",
      "        [210, 210, 210],\n",
      "        [224, 224, 224],\n",
      "        ...,\n",
      "        [220, 220, 220],\n",
      "        [208, 208, 208],\n",
      "        [213, 213, 213]],\n",
      "\n",
      "       [[219, 219, 219],\n",
      "        [225, 225, 225],\n",
      "        [222, 222, 222],\n",
      "        ...,\n",
      "        [220, 220, 220],\n",
      "        [216, 216, 216],\n",
      "        [217, 217, 217]],\n",
      "\n",
      "       [[216, 216, 216],\n",
      "        [230, 230, 230],\n",
      "        [199, 199, 199],\n",
      "        ...,\n",
      "        [192, 192, 192],\n",
      "        [212, 212, 212],\n",
      "        [214, 214, 214]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[219, 219, 219],\n",
      "        [222, 222, 222],\n",
      "        [228, 228, 228],\n",
      "        ...,\n",
      "        [  0,   0,   0],\n",
      "        [255, 255, 255],\n",
      "        [  0,   0,   0]],\n",
      "\n",
      "       [[223, 223, 223],\n",
      "        [227, 227, 227],\n",
      "        [230, 230, 230],\n",
      "        ...,\n",
      "        [  8,   8,   8],\n",
      "        [255, 255, 255],\n",
      "        [ 96,  96,  96]],\n",
      "\n",
      "       [[233, 233, 233],\n",
      "        [235, 235, 235],\n",
      "        [232, 232, 232],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [  0,   0,   0],\n",
      "        [255, 255, 255]]], dtype=uint8)\n",
      "orig_shape: (48, 48)\n",
      "path: '/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg'\n",
      "probs: ultralytics.engine.results.Probs object\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 8.21995735168457, 'inference': 5.090951919555664, 'postprocess': 0.03790855407714844}]\n",
      "Predicted Emotion: happy, Confidence Score: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow  # Assuming the YOLO class is in the yolov5 module\n",
    "import torch\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "\n",
    "# Get project from Roboflow workspace\n",
    "\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "# model = project.version(1).model\n",
    "\n",
    "# Define the path to the weights file\n",
    "weights_path = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train7/weights/best.pt\"\n",
    "\n",
    "# Initialize YOLO model with the weights file\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Perform inference on the image\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "print(predictions)\n",
    "\n",
    "class_labels = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "# Extract the class probabilities from the predictions\n",
    "prediction = predictions[0]  # Assuming a single prediction\n",
    "class_probs = prediction.probs\n",
    "\n",
    "# Find the index of the highest confidence score\n",
    "top1_index = class_probs.top1\n",
    "top1_confidence = class_probs.top1conf\n",
    "\n",
    "# Map the index to the corresponding class label\n",
    "predicted_emotion = class_labels[top1_index]\n",
    "\n",
    "# Print the predicted emotion and its confidence score\n",
    "print(f\"Predicted Emotion: {predicted_emotion}, Confidence Score: {top1_confidence.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Katy Perry\n",
      "Song:  Firework\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Load the CSV file and store the songs in a dictionary\n",
    "songs_by_emotion = {1: [], 2: [], 3: [], 4: []}\n",
    "\n",
    "with open('music.csv', mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        emotion = int(row['Emotion'])\n",
    "        author = row['Author Name']\n",
    "        song = row['Song Name']\n",
    "        songs_by_emotion[emotion].append((author, song))\n",
    "\n",
    "# Map the predicted emotion to the corresponding emotion code\n",
    "if predicted_emotion == 'neutral':\n",
    "    emotion_code = 1\n",
    "elif predicted_emotion == 'happy':\n",
    "    emotion_code = 2\n",
    "elif predicted_emotion == 'sad':\n",
    "    emotion_code = 3\n",
    "elif predicted_emotion == 'angry':\n",
    "    emotion_code = 4\n",
    "\n",
    "# Get a random song from the list of songs corresponding to the predicted emotion\n",
    "random_author, random_song = random.choice(songs_by_emotion[emotion_code])\n",
    "print(\"Author:\", random_author)\n",
    "print(\"Song:\", random_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lcd import LCD\n",
    "\n",
    "# def get_spaces(spaces):\n",
    "#       string = ''\n",
    "#       for i in range(16 - spaces):\n",
    "#          string += ' '\n",
    "#       return string\n",
    "\n",
    "# # Initialize LCD and display the author and the song\n",
    "# lcd = LCD()\n",
    "# lcd.lcd_init()\n",
    "# lcd.send_string(f\"{random_author+get_spaces(len(random_author))}\", lcd.LCD_LINE_1)\n",
    "# lcd.send_string(f\"{random_song+get_spaces(len(random_song))}\", lcd.LCD_LINE_2)\n",
    "# time.sleep(5)\n",
    "# lcd.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Firework\n",
      "Loading file: /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs/Katy Perry Firework.mp3\n",
      "Press Enter to stop playback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 7.0.1 Copyright (c) 2003-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[wav @ 0x122617180] Ignoring maximum wav data size, file may be invalid\n",
      "Input #0, wav, from 'fd:':\n",
      "  Metadata:\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: N/A, bitrate: 1411 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, 2 channels, s16, 1411 kb/s\n",
      "   6.53 M-A:  0.000 fd=   0 aq=  416KB vq=    0KB sq=    0B \r"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Directory containing the MP3 files\n",
    "directory = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs\"  # Change this to your directory\n",
    "\n",
    "# Change this to your actual random song variable\n",
    "print(random_song)\n",
    "\n",
    "# List all files in the directory\n",
    "files_in_directory = os.listdir(directory)\n",
    "\n",
    "# Filter files that contain the song name as a substring\n",
    "matching_files = [file for file in files_in_directory if random_song.lower() in file.lower()]\n",
    "\n",
    "# Print the matching files\n",
    "if matching_files:\n",
    "    file_path = os.path.join(directory, matching_files[0])\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    mp3_file = file_path\n",
    "    \n",
    "    # Command for ffmpeg to convert MP3 to WAV and output to stdout\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \n",
    "        \"-i\", mp3_file, \n",
    "        \"-f\", \"wav\", \n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    # Command for ffplay to read WAV audio from stdin\n",
    "    ffplay_command = [\n",
    "        \"ffplay\", \n",
    "        \"-nodisp\",  # Suppress video display\n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Start ffmpeg subprocess to convert MP3 to WAV and output to stdout\n",
    "        ffmpeg_process = subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "        # Start ffplay subprocess to read WAV audio from stdin\n",
    "        ffplay_process = subprocess.Popen(ffplay_command, stdin=ffmpeg_process.stdout)\n",
    "        \n",
    "        # Wait for the user to exit by pressing Enter\n",
    "        print(\"Press Enter to stop playback...\")\n",
    "        input() # Wait for a single character input (Enter key)\n",
    "        \n",
    "        # Terminate ffplay process\n",
    "        ffplay_process.terminate()\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"No matching files found for the song: {random_song}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
