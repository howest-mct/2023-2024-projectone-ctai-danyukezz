{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def main():\n",
    "    model = YOLO(model=\"yolov8s.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/face_detection-2/data.yaml\", epochs=5, imgsz=(640, 640), verbose=True, batch=8)\n",
    "    model.val()\n",
    "    model.export()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Exporting format folder in progress : 85.0%\n",
      "Version export complete for folder format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in AI-emotion-detection(music-bot)-6 to folder:: 100%|██████████| 482279/482279 [00:37<00:00, 12863.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to AI-emotion-detection(music-bot)-6 in folder:: 100%|██████████| 18695/18695 [00:04<00:00, 4669.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"ai-emotion-detection-music-bot\")\n",
    "version = project.version(6)\n",
    "dataset = version.download(\"folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 🚀 Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train9/weights/best.pt, data=/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6, epochs=50, time=None, patience=100, batch=16, imgsz=(48, 48), save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train10\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/train... found 16293 images in 4 classes ✅ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/test... found 781 images in 4 classes ✅ \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    335364  ultralytics.nn.modules.head.Classify         [256, 4]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1443412 parameters, 1443412 gradients, 3.4 GFLOPs\n",
      "Transferred 158/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train10', view at http://localhost:6006/\n",
      "WARNING ⚠️ updating to 'imgsz=48'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "WARNING ⚠️ imgsz=[48] must be multiple of max stride 32, updating to [64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/train... 16293 images, 0 corrupt: 100%|██████████| 16293/16293 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/test... 781 images, 0 corrupt: 100%|██████████| 781/781 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 64 train, 64 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train10\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G     0.7306          5         64: 100%|██████████| 1019/1019 [08:00<00:00,  2.12it/s] \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.67          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G     0.7196          5         64: 100%|██████████| 1019/1019 [2:30:08<00:00,  8.84s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [07:35<00:00, 18.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       3/50         0G     0.7345          5         64: 100%|██████████| 1019/1019 [5:56:32<00:00, 20.99s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [15:06<00:00, 36.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G     0.7623          5         64: 100%|██████████| 1019/1019 [5:13:52<00:00, 18.48s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [15:43<00:00, 37.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.643          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G     0.7513          5         64: 100%|██████████| 1019/1019 [9:25:55<00:00, 33.32s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:04<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.659          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       6/50         0G     0.7594          5         64: 100%|██████████| 1019/1019 [7:56:08<00:00, 28.04s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.643          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       7/50         0G      0.751          5         64: 100%|██████████| 1019/1019 [5:24:14<00:00, 19.09s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:04<00:00,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.634          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       8/50         0G     0.7488          5         64: 100%|██████████| 1019/1019 [4:44:14<00:00, 16.74s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:45<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.652          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       9/50         0G     0.7474          5         64: 100%|██████████| 1019/1019 [6:26:17<00:00, 22.75s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:04<00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.653          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G     0.7366          5         64: 100%|██████████| 1019/1019 [5:02:22<00:00, 17.80s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.666          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      11/50         0G     0.7326          5         64: 100%|██████████| 1019/1019 [03:39<00:00,  4.65it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.645          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      12/50         0G     0.7333          5         64: 100%|██████████| 1019/1019 [04:11<00:00,  4.06it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.634          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G     0.7212          5         64: 100%|██████████| 1019/1019 [12:06<00:00,  1.40it/s]  \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.643          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G     0.7152          5         64: 100%|██████████| 1019/1019 [04:04<00:00,  4.17it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.644          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      15/50         0G     0.6994          5         64: 100%|██████████| 1019/1019 [03:17<00:00,  5.17it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.641          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      16/50         0G     0.7013          5         64: 100%|██████████| 1019/1019 [03:19<00:00,  5.11it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      17/50         0G     0.7036          5         64: 100%|██████████| 1019/1019 [03:23<00:00,  5.01it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.624          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      18/50         0G     0.6838          5         64: 100%|██████████| 1019/1019 [03:27<00:00,  4.91it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.653          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      19/50         0G     0.6861          5         64: 100%|██████████| 1019/1019 [03:25<00:00,  4.96it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.635          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      20/50         0G     0.6694          5         64: 100%|██████████| 1019/1019 [03:26<00:00,  4.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.636          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      21/50         0G     0.6739          5         64: 100%|██████████| 1019/1019 [03:27<00:00,  4.91it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.65          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      22/50         0G     0.6693          5         64: 100%|██████████| 1019/1019 [03:27<00:00,  4.91it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.648          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      23/50         0G     0.6613          5         64: 100%|██████████| 1019/1019 [03:28<00:00,  4.88it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.631          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      24/50         0G     0.6505          5         64: 100%|██████████| 1019/1019 [03:33<00:00,  4.78it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:07<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.635          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G     0.6484          5         64: 100%|██████████| 1019/1019 [14:15<00:00,  1.19it/s]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.645          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G     0.6378          5         64: 100%|██████████| 1019/1019 [03:23<00:00,  5.01it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.627          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      27/50         0G     0.6441          5         64: 100%|██████████| 1019/1019 [03:24<00:00,  4.99it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.654          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      28/50         0G     0.6631          5         64: 100%|██████████| 1019/1019 [03:36<00:00,  4.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.641          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      29/50         0G     0.6497          5         64: 100%|██████████| 1019/1019 [03:28<00:00,  4.89it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.641          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      30/50         0G     0.6443          5         64: 100%|██████████| 1019/1019 [03:51<00:00,  4.40it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.64          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G     0.6459          5         64: 100%|██████████| 1019/1019 [06:17<00:00,  2.70it/s] \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.64          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G     0.6226          5         64: 100%|██████████| 1019/1019 [03:58<00:00,  4.28it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.636          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G      0.624          5         64: 100%|██████████| 1019/1019 [03:51<00:00,  4.40it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.644          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G     0.6088          5         64: 100%|██████████| 1019/1019 [18:22<00:00,  1.08s/it]  \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.631          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      35/50         0G     0.5999          5         64: 100%|██████████| 1019/1019 [03:29<00:00,  4.86it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.644          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      36/50         0G     0.6006          5         64: 100%|██████████| 1019/1019 [03:33<00:00,  4.78it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:43<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.638          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      37/50         0G     0.5942          5         64: 100%|██████████| 1019/1019 [03:23<00:00,  5.01it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.64          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G     0.5867          5         64: 100%|██████████| 1019/1019 [03:29<00:00,  4.86it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.626          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      39/50         0G     0.5768          5         64: 100%|██████████| 1019/1019 [04:53<00:00,  3.48it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:04<00:00,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.644          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      40/50         0G     0.5651          5         64: 100%|██████████| 1019/1019 [03:21<00:00,  5.05it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.635          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      41/50         0G     0.5646          5         64: 100%|██████████| 1019/1019 [03:29<00:00,  4.86it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.636          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      42/50         0G     0.5462          5         64: 100%|██████████| 1019/1019 [03:31<00:00,  4.81it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.63          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      43/50         0G     0.5381          5         64: 100%|██████████| 1019/1019 [03:31<00:00,  4.82it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.635          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      44/50         0G     0.5386          5         64: 100%|██████████| 1019/1019 [03:32<00:00,  4.79it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.64          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      45/50         0G     0.5227          5         64: 100%|██████████| 1019/1019 [03:39<00:00,  4.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.635          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      46/50         0G      0.509          5         64: 100%|██████████| 1019/1019 [04:22<00:00,  3.88it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:06<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.636          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G     0.5009          5         64: 100%|██████████| 1019/1019 [04:31<00:00,  3.76it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.638          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      48/50         0G     0.4922          5         64: 100%|██████████| 1019/1019 [04:01<00:00,  4.21it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:07<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.639          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G     0.4842          5         64: 100%|██████████| 1019/1019 [04:10<00:00,  4.07it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.643          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      50/50         0G     0.4792          5         64: 100%|██████████| 1019/1019 [03:25<00:00,  4.95it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.643          1\n",
      "\n",
      "50 epochs completed in 57.098 hours.\n",
      "Optimizer stripped from runs/classify/train10/weights/last.pt, 3.0MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs/classify/train10/weights/best.pt, 3.0MB\n",
      "\n",
      "Validating runs/classify/train10/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1440004 parameters, 0 gradients, 3.3 GFLOPs\n",
      "WARNING ⚠️ Dataset 'split=val' not found, using 'split=test' instead.\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/train... found 16293 images in 4 classes ✅ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/test... found 781 images in 4 classes ✅ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [00:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.67          1\n",
      "Speed: 0.0ms preprocess, 3.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/train10\u001b[0m\n",
      "Results saved to \u001b[1mruns/classify/train10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "def main():\n",
    "    model = YOLO(model=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train9/weights/best.pt\")\n",
    "    model.train(data=\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6\", epochs=50, imgsz=(48,48), verbose=True, batch=16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 🚀 Python-3.11.5 torch-2.1.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train7/weights/best.pt, data=/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train11\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/train... found 16293 images in 4 classes ✅ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/test... found 781 images in 4 classes ✅ \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    335364  ultralytics.nn.modules.head.Classify         [256, 4]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1443412 parameters, 1443412 gradients, 3.4 GFLOPs\n",
      "Transferred 158/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train11', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/train... 16293 images, 0 corrupt: 100%|██████████| 16293/16293 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/test... 781 images, 0 corrupt: 100%|██████████| 781/781 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train11\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.394          5        640: 100%|██████████| 1019/1019 [3:52:52<00:00, 13.71s/it]   \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [02:43<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.341          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G       1.29          5        640: 100%|██████████| 1019/1019 [9:20:05<00:00, 32.98s/it]     \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 25/25 [02:38<00:00,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.394          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.162         16        640:  65%|██████▌   | 664/1019 [11:15:58<6:01:24, 61.08s/it]      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train7/weights/best.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m transform \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     14\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     15\u001b[0m     A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     ToTensorV2()\n\u001b[1;32m     40\u001b[0m ])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment_image\u001b[39m(image_path):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/engine/model.py:657\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/engine/trainer.py:213\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/engine/trainer.py:389\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    385\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m    386\u001b[0m     )\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "model = YOLO('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train7/weights/best.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/AI-emotion-detection(music-bot)-6/', \n",
    "    epochs=5, \n",
    "    imgsz=640\n",
    ")\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=45, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(),\n",
    "    ], p=0.2),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "    ], p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(p=0.3),\n",
    "        A.GridDistortion(p=0.1),\n",
    "        A.PiecewiseAffine(p=0.3),\n",
    "    ], p=0.2),\n",
    "    A.OneOf([\n",
    "        A.CLAHE(clip_limit=2),\n",
    "        A.Sharpen(),\n",
    "        A.Emboss(),\n",
    "        A.RandomBrightnessContrast(),\n",
    "    ], p=0.3),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "    A.Resize(height=640, width=640),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def augment_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    augmented = transform(image=image)\n",
    "    return augmented['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7883/startup-events \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.23.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7883/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/03 16:15:44 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.gradio.app/gradio-error-analytics/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[203 203 195]\n",
      "  [197 197 189]\n",
      "  [196 196 188]\n",
      "  ...\n",
      "  [175 174 169]\n",
      "  [187 186 181]\n",
      "  [183 182 177]]\n",
      "\n",
      " [[195 195 187]\n",
      "  [193 193 185]\n",
      "  [196 196 188]\n",
      "  ...\n",
      "  [175 174 169]\n",
      "  [180 179 174]\n",
      "  [172 171 166]]\n",
      "\n",
      " [[192 192 184]\n",
      "  [191 191 183]\n",
      "  [193 193 185]\n",
      "  ...\n",
      "  [182 181 176]\n",
      "  [186 185 180]\n",
      "  [182 181 176]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[167 138 143]\n",
      "  [159 130 135]\n",
      "  [158 129 134]\n",
      "  ...\n",
      "  [236 215 224]\n",
      "  [239 218 227]\n",
      "  [241 220 229]]\n",
      "\n",
      " [[164 135 140]\n",
      "  [163 134 139]\n",
      "  [162 133 138]\n",
      "  ...\n",
      "  [229 208 217]\n",
      "  [232 211 220]\n",
      "  [236 215 224]]\n",
      "\n",
      " [[162 133 138]\n",
      "  [164 135 140]\n",
      "  [165 136 141]\n",
      "  ...\n",
      "  [230 209 218]\n",
      "  [227 206 215]\n",
      "  [230 209 218]]]\n",
      "\n",
      "0: 640x480 1 face, 338.1ms\n",
      "Speed: 10.5ms preprocess, 338.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 64x64 happy 0.86, neutral 0.12, sad 0.02, angry 0.00, 13.8ms\n",
      "Speed: 49.0ms preprocess, 13.8ms inference, 0.3ms postprocess per image at shape (1, 3, 64, 64)\n",
      "None\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/bus.jpg: 640x480 1 face, 338.9ms\n",
      "image 2/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/zidane.jpg: 384x640 (no detections), 344.1ms\n",
      "Speed: 5.7ms preprocess, 341.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1684, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_12326/1734716457.py\", line 23, in predict\n",
      "    original_image = Image.fromarray(image)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3059, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/bus.jpg: 640x480 1 face, 397.0ms\n",
      "image 2/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/zidane.jpg: 384x640 (no detections), 581.6ms\n",
      "Speed: 6.1ms preprocess, 489.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1684, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_12326/1734716457.py\", line 23, in predict\n",
      "    original_image = Image.fromarray(image)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3059, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/bus.jpg: 640x480 1 face, 337.8ms\n",
      "image 2/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/zidane.jpg: 384x640 (no detections), 270.3ms\n",
      "Speed: 5.2ms preprocess, 304.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1684, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_12326/1734716457.py\", line 23, in predict\n",
      "    original_image = Image.fromarray(image)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3059, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2  # Ensure OpenCV is imported\n",
    "\n",
    "# Load your YOLO models (specify the correct path to your models)\n",
    "model1 = YOLO('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/detect/train7/weights/best.pt')\n",
    "model2 = YOLO('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train9/weights/best.pt')\n",
    "\n",
    "def predict(image):\n",
    "    print(image)\n",
    "    result = model1.predict(image)\n",
    "\n",
    "    if not result or len(result[0].boxes) == 0:\n",
    "        return \"No faces detected\"\n",
    "\n",
    "    bounding_box = result[0].boxes[0]  \n",
    "\n",
    "    x1, y1, x2, y2 = bounding_box.xyxy[0].tolist()\n",
    "\n",
    "    # Convert the input image to a PIL Image\n",
    "    original_image = Image.fromarray(image)\n",
    "    \n",
    "    cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # Convert the PIL Image to a NumPy array\n",
    "    cropped_image_np = np.array(cropped_image)\n",
    "\n",
    "    # Convert the image from RGB to BGR (if needed)\n",
    "    cropped_image_np = cv2.cvtColor(cropped_image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Convert the BGR image to grayscale\n",
    "    gray_image = cv2.cvtColor(cropped_image_np, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert the grayscale image back to PIL Image\n",
    "    gray_image_pil = Image.fromarray(gray_image)\n",
    "    gray_image_pil.save(\"cropped_face.jpg\")\n",
    "    \n",
    "    # Convert the input image to a format YOLO can work with\n",
    "    results = model2.predict(np.array(gray_image_pil))\n",
    "    \n",
    "    # Process and format predictions (e.g., bounding boxes, labels, confidence)\n",
    "    output = []\n",
    "    class_labels = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "    # Extract the class probabilities from the predictions\n",
    "    prediction = results[0]  # Assuming a single prediction\n",
    "    class_probs = prediction.probs\n",
    "\n",
    "    # Find the index of the highest confidence score\n",
    "    top1_index = class_probs.top1\n",
    "    top1_confidence = class_probs.top1conf\n",
    "\n",
    "    # Map the index to the corresponding class label\n",
    "    predicted_emotion = class_labels[top1_index]\n",
    "\n",
    "    # Print the predicted emotion and its confidence score\n",
    "    output.append(f\"Predicted Emotion: {predicted_emotion}, Confidence Score: {top1_confidence.item():.2f}\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Input Image\", sources=[\"upload\", \"webcam\"]),\n",
    "    outputs=gr.Textbox(label=\"Predictions\"),\n",
    "    title=\"YOLO Model Prediction\",\n",
    "    description=\"Upload an image to get predictions from the YOLO model\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to take a screenshot from the camera feed\n",
    "def take_screenshot():\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Save the frame as an image\n",
    "        cv2.imwrite('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg', frame)\n",
    "    else:\n",
    "        print(\"Failed to capture frame from the camera feed\")\n",
    "\n",
    "# Start the camera feed\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to the appropriate camera index if needed\n",
    "\n",
    "# Load your trained model\n",
    "# model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI model exam/face_recognition/runs/detect/train5/weights/best.pt\")\n",
    "\n",
    "# Listen for key presses\n",
    "while True:\n",
    "    # Capture a frame from the camera feed\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(' '):  # Space key press\n",
    "        take_screenshot()\n",
    "        break\n",
    "    elif key == ord('q'):  # Q key press\n",
    "        break\n",
    "\n",
    "# Close the camera feed and all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7883/startup-events \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.23.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7883/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/03 16:15:44 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.gradio.app/gradio-error-analytics/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[203 203 195]\n",
      "  [197 197 189]\n",
      "  [196 196 188]\n",
      "  ...\n",
      "  [175 174 169]\n",
      "  [187 186 181]\n",
      "  [183 182 177]]\n",
      "\n",
      " [[195 195 187]\n",
      "  [193 193 185]\n",
      "  [196 196 188]\n",
      "  ...\n",
      "  [175 174 169]\n",
      "  [180 179 174]\n",
      "  [172 171 166]]\n",
      "\n",
      " [[192 192 184]\n",
      "  [191 191 183]\n",
      "  [193 193 185]\n",
      "  ...\n",
      "  [182 181 176]\n",
      "  [186 185 180]\n",
      "  [182 181 176]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[167 138 143]\n",
      "  [159 130 135]\n",
      "  [158 129 134]\n",
      "  ...\n",
      "  [236 215 224]\n",
      "  [239 218 227]\n",
      "  [241 220 229]]\n",
      "\n",
      " [[164 135 140]\n",
      "  [163 134 139]\n",
      "  [162 133 138]\n",
      "  ...\n",
      "  [229 208 217]\n",
      "  [232 211 220]\n",
      "  [236 215 224]]\n",
      "\n",
      " [[162 133 138]\n",
      "  [164 135 140]\n",
      "  [165 136 141]\n",
      "  ...\n",
      "  [230 209 218]\n",
      "  [227 206 215]\n",
      "  [230 209 218]]]\n",
      "\n",
      "0: 640x480 1 face, 338.1ms\n",
      "Speed: 10.5ms preprocess, 338.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 64x64 happy 0.86, neutral 0.12, sad 0.02, angry 0.00, 13.8ms\n",
      "Speed: 49.0ms preprocess, 13.8ms inference, 0.3ms postprocess per image at shape (1, 3, 64, 64)\n",
      "None\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/bus.jpg: 640x480 1 face, 338.9ms\n",
      "image 2/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/zidane.jpg: 384x640 (no detections), 344.1ms\n",
      "Speed: 5.7ms preprocess, 341.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1684, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_12326/1734716457.py\", line 23, in predict\n",
      "    original_image = Image.fromarray(image)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3059, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/bus.jpg: 640x480 1 face, 397.0ms\n",
      "image 2/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/zidane.jpg: 384x640 (no detections), 581.6ms\n",
      "Speed: 6.1ms preprocess, 489.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1684, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_12326/1734716457.py\", line 23, in predict\n",
      "    original_image = Image.fromarray(image)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3059, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/bus.jpg: 640x480 1 face, 337.8ms\n",
      "image 2/2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ultralytics/assets/zidane.jpg: 384x640 (no detections), 270.3ms\n",
      "Speed: 5.2ms preprocess, 304.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1684, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/fb/p7zywq8d7351kf4l4fwh2dmw0000gn/T/ipykernel_12326/1734716457.py\", line 23, in predict\n",
      "    original_image = Image.fromarray(image)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3059, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2  # Ensure OpenCV is imported\n",
    "\n",
    "# Load your YOLO models (specify the correct path to your models)\n",
    "model1 = YOLO('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/detect/train7/weights/best.pt')\n",
    "model2 = YOLO('/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train9/weights/best.pt')\n",
    "\n",
    "def predict(image):\n",
    "    print(image)\n",
    "    result = model1.predict(image)\n",
    "\n",
    "    if not result or len(result[0].boxes) == 0:\n",
    "        return \"No faces detected\"\n",
    "\n",
    "    bounding_box = result[0].boxes[0]  \n",
    "\n",
    "    x1, y1, x2, y2 = bounding_box.xyxy[0].tolist()\n",
    "\n",
    "    # Convert the input image to a PIL Image\n",
    "    original_image = Image.fromarray(image)\n",
    "    \n",
    "    cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # Convert the PIL Image to a NumPy array\n",
    "    cropped_image_np = np.array(cropped_image)\n",
    "\n",
    "    # Convert the image from RGB to BGR (if needed)\n",
    "    cropped_image_np = cv2.cvtColor(cropped_image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Convert the BGR image to grayscale\n",
    "    gray_image = cv2.cvtColor(cropped_image_np, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert the grayscale image back to PIL Image\n",
    "    gray_image_pil = Image.fromarray(gray_image)\n",
    "    gray_image_pil.save(\"cropped_face.jpg\")\n",
    "    \n",
    "    # Convert the input image to a format YOLO can work with\n",
    "    results = model2.predict(np.array(gray_image_pil))\n",
    "    \n",
    "    # Process and format predictions (e.g., bounding boxes, labels, confidence)\n",
    "    output = []\n",
    "    class_labels = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "    # Extract the class probabilities from the predictions\n",
    "    prediction = results[0]  # Assuming a single prediction\n",
    "    class_probs = prediction.probs\n",
    "\n",
    "    # Find the index of the highest confidence score\n",
    "    top1_index = class_probs.top1\n",
    "    top1_confidence = class_probs.top1conf\n",
    "\n",
    "    # Map the index to the corresponding class label\n",
    "    predicted_emotion = class_labels[top1_index]\n",
    "\n",
    "    # Print the predicted emotion and its confidence score\n",
    "    output.append(f\"Predicted Emotion: {predicted_emotion}, Confidence Score: {top1_confidence.item():.2f}\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Input Image\", sources=[\"upload\", \"webcam\"]),\n",
    "    outputs=gr.Textbox(label=\"Predictions\"),\n",
    "    title=\"YOLO Model Prediction\",\n",
    "    description=\"Upload an image to get predictions from the YOLO model\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/screenshot.jpg: 384x640 2 faces, 185.1ms\n",
      "Speed: 8.0ms preprocess, 185.1ms inference, 14.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Roboflow and download the model\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace(\"danyukezz\").project(\"face_detection-y52o3\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "\n",
    "model = YOLO(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/detect/train7/weights/best.pt\")\n",
    "\n",
    "\n",
    "path = \"screenshot.jpg\"\n",
    "result = model.predict(path)\n",
    "\n",
    "\n",
    "bounding_box = result[0].boxes[0]  \n",
    "\n",
    "x1, y1, x2, y2 = bounding_box.xyxy[0].tolist()\n",
    "\n",
    "original_image = PILImage.open(path)\n",
    "\n",
    "cropped_image = original_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "# cropped_image = cropped_image.resize((48,48), resample=Image.BILINEAR)\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "cropped_image_np = np.array(cropped_image)\n",
    "\n",
    "# Convert the image from RGB to BGR (if needed)\n",
    "cropped_image_np = cv2.cvtColor(cropped_image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Convert the BGR image to grayscale\n",
    "gray_image = cv2.cvtColor(cropped_image_np, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the grayscale image back to PIL Image\n",
    "gray_image_pil = Image.fromarray(gray_image)\n",
    "\n",
    "# Save the grayscale image\n",
    "gray_image_pil.save(\"cropped_face.jpg\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"cropped_face.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import supervision as sv\n",
    "\n",
    "# print(model.names)\n",
    "# def get_keys_from_value(d, val):\n",
    "#     return [k for k, v in d.items() if v == val]\n",
    "\n",
    "# detections = sv.Detections.from_ultralytics(result)\n",
    "# detections = detections[detections.class_id == get_keys_from_value(model.names, 'face')] # if necessary replace by your own class\n",
    "# print(\"Amount of faces: \",len(detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL.Image as Image\n",
    "# import gradio as gr\n",
    "\n",
    "# from ultralytics import ASSETS\n",
    "\n",
    "# def predict_image(img, conf_threshold, iou_threshold):\n",
    "#     results = model.predict(\n",
    "#         source=img,\n",
    "#         conf=conf_threshold,\n",
    "#         iou=iou_threshold,\n",
    "#         show_labels=True,\n",
    "#         show_conf=True,\n",
    "#         imgsz=640,\n",
    "#         agnostic_nms=True\n",
    "#     )\n",
    "\n",
    "#     for r in results:\n",
    "#         im_array = r.plot()\n",
    "#         im = Image.fromarray(im_array[..., ::-1])\n",
    "\n",
    "#     result = model.predict(img)\n",
    "#     result = result[0] if isinstance(result, list) else result \n",
    "#     detections = sv.Detections.from_ultralytics(result)\n",
    "#     detections = detections[detections.class_id == get_keys_from_value(model.names, 'Drone')]\n",
    "\n",
    "#     print(results)\n",
    "#     return im, {len(detections)}\n",
    "\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=predict_image,\n",
    "#     inputs=[\n",
    "#         gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence threshold\"),\n",
    "#         gr.Slider(minimum=0, maximum=1, value=0.45, label=\"IoU threshold\")\n",
    "#     ],\n",
    "#     outputs=[gr.Image(type=\"pil\", label=\"Result\"), gr.Textbox(label=\"Amount of drones\")],\n",
    "#     title=\"Ultralytics Gradio\",\n",
    "#     description=\"Upload images for inference. The Ultralytics YOLOv8n model is used by default.\",\n",
    "#     theme = gr.themes.Monochrome()\n",
    "\n",
    "# )\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "{\n",
      "  \"time\": 0.12032753700032117,\n",
      "  \"image\": {\n",
      "    \"width\": 397,\n",
      "    \"height\": 275\n",
      "  },\n",
      "  \"predictions\": {\n",
      "    \"angry\": {\n",
      "      \"confidence\": 0.026481078937649727\n",
      "    },\n",
      "    \"happy\": {\n",
      "      \"confidence\": 0.008471661247313023\n",
      "    },\n",
      "    \"neutral\": {\n",
      "      \"confidence\": 0.6760207414627075\n",
      "    },\n",
      "    \"sad\": {\n",
      "      \"confidence\": 0.38206490874290466\n",
      "    }\n",
      "  },\n",
      "  \"predicted_classes\": [\n",
      "    \"neutral\"\n",
      "  ],\n",
      "  \"image_path\": \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\",\n",
      "  \"prediction_type\": \"ClassificationModel\"\n",
      "}\n",
      "\n",
      "\n",
      "Predicted Emotion with Highest Confidence: neutral, Confidence Score: 0.6760207414627075\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "model = project.version(1).model\n",
    "\n",
    "# infer on a local image\n",
    "print(model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\"))\n",
    "\n",
    "# visualize your prediction\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "prediction = predictions[0]\n",
    "\n",
    "# Extract predicted emotions and their confidence scores\n",
    "predicted_emotions = prediction['predictions']\n",
    "\n",
    "# Find the emotion with the highest confidence score\n",
    "max_confidence = 0\n",
    "predicted_emotion = None\n",
    "\n",
    "for emotion, info in predicted_emotions.items():\n",
    "    confidence_score = info['confidence']\n",
    "    if confidence_score > max_confidence:\n",
    "        max_confidence = confidence_score\n",
    "        predicted_emotion = emotion\n",
    "\n",
    "# Print the predicted emotion with the highest confidence score\n",
    "print(f\"Predicted Emotion with Highest Confidence: {predicted_emotion}, Confidence Score: {max_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "\n",
      "image 1/1 /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg: 64x64 happy 0.65, neutral 0.32, sad 0.02, angry 0.01, 7.4ms\n",
      "Speed: 12.0ms preprocess, 7.4ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: None\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'angry', 1: 'happy', 2: 'neutral', 3: 'sad'}\n",
      "obb: None\n",
      "orig_img: array([[[206, 206, 206],\n",
      "        [207, 207, 207],\n",
      "        [207, 207, 207],\n",
      "        ...,\n",
      "        [210, 210, 210],\n",
      "        [208, 208, 208],\n",
      "        [207, 207, 207]],\n",
      "\n",
      "       [[209, 209, 209],\n",
      "        [209, 209, 209],\n",
      "        [209, 209, 209],\n",
      "        ...,\n",
      "        [210, 210, 210],\n",
      "        [208, 208, 208],\n",
      "        [207, 207, 207]],\n",
      "\n",
      "       [[212, 212, 212],\n",
      "        [212, 212, 212],\n",
      "        [212, 212, 212],\n",
      "        ...,\n",
      "        [208, 208, 208],\n",
      "        [207, 207, 207],\n",
      "        [206, 206, 206]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 60,  60,  60],\n",
      "        [ 54,  54,  54],\n",
      "        [ 49,  49,  49],\n",
      "        ...,\n",
      "        [122, 122, 122],\n",
      "        [138, 138, 138],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[ 63,  63,  63],\n",
      "        [ 51,  51,  51],\n",
      "        [ 42,  42,  42],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [149, 149, 149]],\n",
      "\n",
      "       [[ 63,  63,  63],\n",
      "        [ 51,  51,  51],\n",
      "        [ 42,  42,  42],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [  0,   0,   0]]], dtype=uint8)\n",
      "orig_shape: (292, 206)\n",
      "path: '/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg'\n",
      "probs: ultralytics.engine.results.Probs object\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 12.013912200927734, 'inference': 7.43865966796875, 'postprocess': 0.080108642578125}]\n",
      "Predicted Emotion: happy, Confidence Score: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow  # Assuming the YOLO class is in the yolov5 module\n",
    "import torch\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"Bb2wv19Rp5UGNJW71H9j\")\n",
    "\n",
    "# Get project from Roboflow workspace\n",
    "\n",
    "project = rf.workspace().project(\"ai-emotion-detection-music-bot\")\n",
    "# model = project.version(1).model\n",
    "\n",
    "# Define the path to the weights file\n",
    "weights_path = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/runs/classify/train9/weights/best.pt\"\n",
    "\n",
    "# Initialize YOLO model with the weights file\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Perform inference on the image\n",
    "predictions = model.predict(\"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/cropped_face.jpg\")\n",
    "print(predictions)\n",
    "\n",
    "class_labels = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "# Extract the class probabilities from the predictions\n",
    "prediction = predictions[0]  # Assuming a single prediction\n",
    "class_probs = prediction.probs\n",
    "\n",
    "# Find the index of the highest confidence score\n",
    "top1_index = class_probs.top1\n",
    "top1_confidence = class_probs.top1conf\n",
    "\n",
    "# Map the index to the corresponding class label\n",
    "predicted_emotion = class_labels[top1_index]\n",
    "\n",
    "# Print the predicted emotion and its confidence score\n",
    "print(f\"Predicted Emotion: {predicted_emotion}, Confidence Score: {top1_confidence.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Justin Timberlake\n",
      "Song: Can't Stop the Feeling!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Load the CSV file and store the songs in a dictionary\n",
    "songs_by_emotion = {1: [], 2: [], 3: [], 4: []}\n",
    "\n",
    "with open('music.csv', mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        emotion = int(row['Emotion'])\n",
    "        author = row['Author Name']\n",
    "        song = row['Song Name']\n",
    "        songs_by_emotion[emotion].append((author, song))\n",
    "\n",
    "# Map the predicted emotion to the corresponding emotion code\n",
    "if predicted_emotion == 'neutral':\n",
    "    emotion_code = 1\n",
    "elif predicted_emotion == 'happy':\n",
    "    emotion_code = 2\n",
    "elif predicted_emotion == 'sad':\n",
    "    emotion_code = 3\n",
    "elif predicted_emotion == 'angry':\n",
    "    emotion_code = 4\n",
    "\n",
    "# Get a random song from the list of songs corresponding to the predicted emotion\n",
    "random_author, random_song = random.choice(songs_by_emotion[emotion_code])\n",
    "print(\"Author:\", random_author)\n",
    "print(\"Song:\", random_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lcd import LCD\n",
    "\n",
    "# def get_spaces(spaces):\n",
    "#       string = ''\n",
    "#       for i in range(16 - spaces):\n",
    "#          string += ' '\n",
    "#       return string\n",
    "\n",
    "# # Initialize LCD and display the author and the song\n",
    "# lcd = LCD()\n",
    "# lcd.lcd_init()\n",
    "# lcd.send_string(f\"{random_author+get_spaces(len(random_author))}\", lcd.LCD_LINE_1)\n",
    "# lcd.send_string(f\"{random_song+get_spaces(len(random_song))}\", lcd.LCD_LINE_2)\n",
    "# time.sleep(5)\n",
    "# lcd.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't Stop the Feeling!\n",
      "Loading file: /Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs/Justin Timberlake Can't Stop The Feeling! (From Dreamworks Animation's _Trolls_).mp3\n",
      "Press Enter to stop playback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 7.0.1 Copyright (c) 2003-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[wav @ 0x151b1d4a0] Ignoring maximum wav data size, file may be invalid\n",
      "Input #0, wav, from 'fd:':0 aq=    0KB vq=    0KB sq=    0B \n",
      "  Metadata:\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: N/A, bitrate: 1411 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, 2 channels, s16, 1411 kb/s\n",
      "   2.09 M-A:  0.000 fd=   0 aq=  400KB vq=    0KB sq=    0B \r"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Directory containing the MP3 files\n",
    "directory = \"/Users/danyukezz/Desktop/1 year 2 semester/project one/2023-2024-projectone-ctai-danyukezz/AI/AI model exam/face_recognition/songs\"  # Change this to your directory\n",
    "\n",
    "# Change this to your actual random song variable\n",
    "print(random_song)\n",
    "\n",
    "# List all files in the directory\n",
    "files_in_directory = os.listdir(directory)\n",
    "\n",
    "# Filter files that contain the song name as a substring\n",
    "matching_files = [file for file in files_in_directory if random_song.lower() in file.lower()]\n",
    "\n",
    "# Print the matching files\n",
    "if matching_files:\n",
    "    file_path = os.path.join(directory, matching_files[0])\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    mp3_file = file_path\n",
    "    \n",
    "    # Command for ffmpeg to convert MP3 to WAV and output to stdout\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \n",
    "        \"-i\", mp3_file, \n",
    "        \"-f\", \"wav\", \n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    # Command for ffplay to read WAV audio from stdin\n",
    "    ffplay_command = [\n",
    "        \"ffplay\", \n",
    "        \"-nodisp\",  # Suppress video display\n",
    "        \"-\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Start ffmpeg subprocess to convert MP3 to WAV and output to stdout\n",
    "        ffmpeg_process = subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "        # Start ffplay subprocess to read WAV audio from stdin\n",
    "        ffplay_process = subprocess.Popen(ffplay_command, stdin=ffmpeg_process.stdout)\n",
    "        \n",
    "        # Wait for the user to exit by pressing Enter\n",
    "        print(\"Press Enter to stop playback...\")\n",
    "        input() # Wait for a single character input (Enter key)\n",
    "        \n",
    "        # Terminate ffplay process\n",
    "        ffplay_process.terminate()\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"No matching files found for the song: {random_song}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
